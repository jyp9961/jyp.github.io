<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yunpeng Jiang</title>

    <meta name="author" content="Yunpeng Jiang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yunpeng Jiang
                </p>
                <p>
                  I'm a PhD student at <a href="https://www.ji.sjtu.edu.cn/">UM-SJTU Joint Institute</a> in Shanghai,
                  working on sample-efficient deep reinforcement learning.
                  I am currently advised by <a href="https://people.csail.mit.edu/yban/index.html">Yutong Ban</a> and
                  <a href="https://weng.fr/">Paul Weng</a>. Before my PhD studies, I completed my bachelor's degree in electrical and computer engineering at the
                  same institute</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:jyp9961@sjtu.edu.cn">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=9l3RYXkAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/jyp9961/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/yunpeng_jiang.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/yunpeng_jiang.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in deep reinforcement learning and robot manipulation. My current research focuses on
                  improving <strong>sample-efficiency</strong> of robot learning algorithms by
                  exploiting <strong>data augmentation</strong> and leveraging <strong>equivariance</strong>.
                    Some papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="new_data/NIPS2025/teaser.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2505.13925">
            <span class="papertitle">Time Reversal Symmetry for Efficient Robotic Manipulations in Deep Reinforcement Learning
            </span>
              </a>
              <br>
              <strong>Yunpeng Jiang</strong>,
              <a href="https://jianshu-hu.github.io/">Jianshu Hu</a>,
              <a href="https://weng.fr/">Paul Weng</a>,
              <a href="https://people.csail.mit.edu/yban/index.html">Yutong Ban</a>,

              <br>
              <em>Under Review</em>
              <br>
              <a href="https://arxiv.org/abs/2505.13925">arXiv</a>
              <p></p>
              <p>
                We propose Time Reversal symmetry enhanced Deep Reinforcement Learning (TR-DRL), a framework that combines trajectory reversal augmentation and time reversal guided reward shaping to efficiently solve temporally symmetric tasks.
              </p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="new_data/TMLR2025/teaser.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2407.03146">
            <span class="papertitle">Understanding and Reducing the Class-Dependent Effects of Data Augmentation with A Two-Player Game Approach
            </span>
              </a>
              <br>
                  <strong>Yunpeng Jiang</strong>,
              <a href="https://people.csail.mit.edu/yban/index.html">Yutong Ban</a>,
              <a href="https://weng.fr/">Paul Weng</a>,

              <br>
              <em>TMLR</em>, 06/2025
              <br>
            <!--        <a href="https://cat3d.github.io/">project page</a>-->
            <!--        /-->
              <a href="https://arxiv.org/abs/2407.03146">arXiv</a>, 
              <a href="https://jyp9961.github.io/CLAM_project_page/">project page</a>
              <p></p>
              <p>
                We reformulate classification using a novel variant of fair optimization and propose a multiplicative weight optimization method to reduce the class-dependent effects of data augmentation.
              </p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="new_data/ICLR2024/method.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2402.12181">
            <span class="papertitle">Revisiting Data Augmentation in Deep Reinforcement Learning
            </span>
              </a>
              <br>
              <a href="https://jianshu-hu.github.io/">Jianshu Hu</a>,
                  <strong>Yunpeng Jiang</strong>,
              <a href="https://weng.fr/">Paul Weng</a>,

              <br>
              <em>ICLR</em>, 2024
              <br>
            <!--        <a href="https://cat3d.github.io/">project page</a>-->
            <!--        /-->
              <a href="https://arxiv.org/abs/2402.12181">arXiv</a>
              <p></p>
              <p>
                We make recommendations on how to exploit data augmentation in image-based DRL in a more principled way.
                And we include a novel regularization term called tangent prop in RL training.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="new_data/AI4AD@IJCAI-ECAI2022/figure1.png" alt="b3do" width="160" style="border-style: none">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/pdf?id=rRDr9z2LrJq">
            <span class="papertitle">An Interpretable Deep Reinforcement Learning Approach to Autonomous Driving
            </span>
              </a>
              <br>
              Zhihao Song,              
              <strong>Yunpeng Jiang</strong>,
              Jianyi Zhang, 
              <a href="https://weng.fr/">Paul Weng</a>,
              Dong Li,
              Wulong Liu,
              Jianye Hao
              
              <br>
              <em>AI4AD @ IJCAI-ECAI </em>, 2022
              <br>
            <!--        <a href="https://cat3d.github.io/">project page</a>-->
            <!--        /-->
              <a href="https://openreview.net/pdf?id=rRDr9z2LrJq">pdf</a>
              <p></p>
              <p>
                We exploit a recent neuro-symbolic model called differentiable logic machine to learn an interpretable controller in the form of a first-order logic program.
                We demonstrate the feasibility of our approach on two classical decision-making scenarios in autonomous driving: lane changing and intersection management.
              </p>
            </td>
          </tr>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This website is build on this <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. Thanks to <a href="https://jonbarron.info/">Jon Barron.</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>